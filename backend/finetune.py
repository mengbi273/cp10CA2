import os
import json
import shutil
import random

# === é…ç½®å‚æ•° ===
json_file = "annotations.json"       # åŸå§‹ JSON æ–‡ä»¶è·¯å¾„
image_base_dir = "images"          # å›¾ç‰‡ç›®å½•ï¼ˆåŸå§‹å›¾ç‰‡è·¯å¾„æŒ‡å‘è¿™ä¸ªæ–‡ä»¶å¤¹ï¼‰
output_dir = "output"              # è¾“å‡ºç›®å½•ï¼Œä¼šè‡ªåŠ¨åˆ›å»º

# === åˆ›å»ºè¾“å‡ºå­æ–‡ä»¶å¤¹ ===
os.makedirs(output_dir, exist_ok=True)
for split in ["train", "val", "test"]:
    os.makedirs(os.path.join(output_dir, "images", split), exist_ok=True)

# === è¯»å– JSON æ–‡ä»¶ ===
with open(json_file, "r", encoding="utf-8") as f:
    data = json.load(f)

# === åªä¿ç•™å¿…è¦å­—æ®µ ===
filtered_data = [{"image_path": item["image_path"], "product_title": item["product_title"]} for item in data]

# === æ‰“ä¹±å¹¶åˆ’åˆ†æ•°æ®é›† ===
random.shuffle(filtered_data)
n = len(filtered_data)
train_data = filtered_data[:int(0.7 * n)]
val_data = filtered_data[int(0.7 * n):int(0.85 * n)]
test_data = filtered_data[int(0.85 * n):]

# === ä¿å­˜ä¸º JSON æ–‡ä»¶ ===
def save_json(data, filename):
    with open(os.path.join(output_dir, filename), "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2)

save_json(train_data, "train.json")
save_json(val_data, "val.json")
save_json(test_data, "test.json")

# === å¤åˆ¶å›¾ç‰‡åˆ°å¯¹åº”ç›®å½• ===
def copy_images(data, split):
    for item in data:
        src_path = os.path.join(image_base_dir, os.path.basename(item["image_path"]))
        dst_path = os.path.join(output_dir, "images", split, os.path.basename(item["image_path"]))
        if os.path.exists(src_path):
            shutil.copy(src_path, dst_path)
        else:
            print(f"âš ï¸ å›¾ç‰‡ä¸å­˜åœ¨: {src_path}")

copy_images(train_data, "train")
copy_images(val_data, "val")
copy_images(test_data, "test")


import json
from PIL import Image
import torch
import torch.nn as nn
import clip
from transformers import CLIPProcessor,CLIPModel
from tqdm import tqdm
import os
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader

model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
# Setting our device to GPU (Cuda) and loading the pre-trained CLIP model.

device = "cuda:0" if torch.cuda.is_available() else "cpu" 

model, preprocess = clip.load("ViT-B/32", device=device, jit=False)

optimizer = torch.optim.Adam(
    model.parameters(), lr=5e-5, betas=(0.9, 0.98), eps=1e-6 ,weight_decay=0.2) 

# ä¸ºæ˜¾å­˜ä¼˜åŒ–è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå¿…é¡»åœ¨å¯¼å…¥ torch å‰è®¾ç½®ï¼‰
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

# è¶…å‚æ•°
BATCH_SIZE = 32
NUM_EPOCHS = 2
LR = 5e-6
MAX_GRAD_NORM = 1.0
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# åŠ è½½ CLIP æ¨¡å‹å’Œé¢„å¤„ç†å™¨
model, preprocess = clip.load("ViT-B/32", device=DEVICE, jit=False)
model = model.to(DEVICE)
model = model.float() 
model.train()

# åŠ è½½è®­ç»ƒæ•°æ®
json_path = "./train_data.json"
image_root = "./images/train/"

input_data = []
with open(json_path, 'r') as f:
    for line in f:
        obj = json.loads(line)
        input_data.append(obj)

# åŠ è½½éªŒè¯æ•°æ®
val_json_path = "./val_data.json"
val_image_root = "./images/val/"

val_data = []
with open(val_json_path, 'r') as f:
    for line in f:
        val_data.append(json.loads(line))

# åŠ è½½éªŒè¯æ•°æ®
test_json_path = "./test_data.json"
test_image_root = "./images/test/"

test_data = []
with open(test_json_path, 'r') as f:
    for line in f:
        test_data.append(json.loads(line))

def evaluate(model, val_dataset, topk=(1, 5, 10, 20)):
    model.eval()
    with torch.no_grad():
        # Step 1: æå–æ‰€æœ‰å›¾ç‰‡ç‰¹å¾
        all_image_features = []
        image_paths = []

        for image, _ in DataLoader(val_dataset, batch_size=64):
            image = image.to(DEVICE)
            with torch.amp.autocast(DEVICE):
                image_feat = model.encode_image(image)
                image_feat /= image_feat.norm(dim=-1, keepdim=True)
            all_image_features.append(image_feat)

        image_features = torch.cat(all_image_features, dim=0)  # (N_img, D)

        # Step 2: å¯¹æ¯ä¸ªæ–‡æœ¬æå–ç‰¹å¾ï¼Œå¹¶åŒ¹é…æ‰€æœ‰å›¾åƒ
        topk_correct = {k: 0 for k in topk}
        total = len(val_dataset)

        for idx in range(total):
            text = val_dataset.tokenized_texts[idx].unsqueeze(0).to(DEVICE)  # (1, context_length)
            with torch.amp.autocast(DEVICE):
                text_feat = model.encode_text(text)
                text_feat /= text_feat.norm(dim=-1, keepdim=True)

            similarity = text_feat @ image_features.T  # (1, N_img)
            values, indices = similarity.topk(max(topk), dim=-1)  # indices: (1, topk)

            for k in topk:
                if idx in indices[0][:k]:
                    topk_correct[k] += 1

        accs = {f"top{k}": topk_correct[k] / total for k in topk}
        return accs


# æ„å»º Dataset
class FashionDataset(Dataset):
    def __init__(self, data, image_root):
        self.image_paths = []
        self.texts = []

        for item in data:
            img_name = os.path.basename(item['image_path'])
            img_path = os.path.join(image_root, img_name)

            if not os.path.exists(img_path):
                continue

            # ä½¿ç”¨ class label æ„é€ æ›´æ¸…æ™°çš„æ–‡æœ¬æè¿°
            #caption = f"a photo of a {item['class_label']}"

            # ä½¿ç”¨ product_titleä½œä¸ºcaptionï¼Œå–å‰77é•¿åº¦
            caption = f"{item['product_title'][:77]}"
            self.image_paths.append(img_path)
            self.texts.append(caption)

        # ä¸€æ¬¡æ€§ tokenize æ–‡æœ¬
        self.tokenized_texts = clip.tokenize(self.texts)

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        image = preprocess(Image.open(self.image_paths[idx]).convert("RGB"))
        text = self.tokenized_texts[idx]
        return image, text
    
dataset = FashionDataset(input_data, image_root)
val_dataset = FashionDataset(val_data, val_image_root)
test_dataset = FashionDataset(test_data, test_image_root)

train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)

# Loss & Optimizer
loss_img = nn.CrossEntropyLoss()
loss_txt = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=LR, betas=(0.9, 0.98), eps=1e-6 ,weight_decay=0.2)
scaler = torch.amp.GradScaler()

# å¼€å§‹è®­ç»ƒ
val_top1_list = []
val_top5_list = []

for epoch in range(NUM_EPOCHS):
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS}")

    for images, texts in pbar:
        images, texts = images.to(DEVICE), texts.to(DEVICE)
        optimizer.zero_grad()
        ground_truth = torch.arange(len(images), device=DEVICE)

        with torch.amp.autocast(DEVICE):
            logits_per_image, logits_per_text = model(images, texts)
            loss = (loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text, ground_truth)) / 2

        scaler.scale(loss).backward()

        # æ¢¯åº¦è£å‰ª
        scaler.unscale_(optimizer)
        nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)

        scaler.step(optimizer)
        scaler.update()
        torch.cuda.empty_cache()

        # è®¡ç®— top-1 accï¼ˆå›¾åƒ -> æ–‡æœ¬ï¼‰
        _, pred = logits_per_image.softmax(dim=-1).max(dim=-1)
        correct += (pred == ground_truth).sum().item()
        total += len(images)

        total_loss += loss.item()
        pbar.set_postfix(loss=loss.item(), acc=f"{100*correct/total:.2f}%")

    # ä¿å­˜æ¨¡å‹
    torch.save({
        "epoch": epoch,
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict()
    }, f"finetuned_clip_epoch{epoch+1}.pt")

    print(f"âœ… Epoch {epoch+1} done. Loss: {total_loss/len(train_loader):.4f}, Acc: {100*correct/total:.2f}%")

    # éªŒè¯é˜¶æ®µ
    accs = evaluate(model, val_dataset)
    val_top1_list.append(accs["top1"])
    val_top5_list.append(accs["top5"])

    print(f"ğŸ“Š Validation top1: {accs['top1']*100:.2f}%, top5: {accs['top5']*100:.2f}%")

# === ç»˜å›¾ ===
plt.plot(range(1, NUM_EPOCHS + 1), [a * 100 for a in val_top1_list], label='Top-1 Accuracy')
plt.plot(range(1, NUM_EPOCHS + 1), [a * 100 for a in val_top5_list], label='Top-5 Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy (%)')
plt.title('Validation Accuracy over Epochs')
plt.legend()
plt.grid()
plt.savefig("val_accuracy.png")
plt.show()

# è®¾å¤‡é…ç½®
device = "cuda" if torch.cuda.is_available() else "cpu"

# åŠ è½½æ¨¡å‹å’Œé¢„å¤„ç†å™¨
model, preprocess = clip.load("ViT-B/32", device=device)

checkpoint = torch.load("finetuned_clip_epoch2.pt")
model.load_state_dict(checkpoint['model_state_dict'])

# åŠ è½½æ¨¡å‹å’Œé¢„å¤„ç†å™¨
raw_model, raw_preprocess = clip.load("ViT-B/32", device=device)

accs = evaluate(raw_model, test_dataset)
print("å¾®è°ƒå‰topkå‡†ç¡®ç‡ä¸ºï¼š", accs)

accs = evaluate(raw_model, test_dataset)
print("å¾®è°ƒå‰topkå‡†ç¡®ç‡ä¸ºï¼š", accs)
